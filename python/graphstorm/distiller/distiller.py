"""
    Copyright 2023 Contributors

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

    Distill wrapper.
"""

import torch as th
from torch.nn.parallel import DistributedDataParallel
import os
import json
import pandas as pd

from ..model.gnn import do_full_graph_inference
from ..utils import sys_tracker
from .utils import remap_embeddings

from ..model.gnn_distill import GSDistilledModel
from ..dataloading import DataloaderGenerator, DataProvider

class GSdistiller():
    """ GNN distiller.

    Parameters
    ----------
    model : GSgnnLinkPredictionModelBase
        The GNN model for GNN embeddings inference.
    rank : int
        The rank.
    config : GSConfig
        Configs for GNN distillation
    """
    def __init__(self, model, rank, config):
        self._rank = rank
        self._model = model
        self._device = -1
        self._evaluator = None
        self._task_tracker = None
        self._config = config

    def infer(self, data, loader,
            edge_mask_for_gnn_embeddings='train_mask',
            node_id_mapping_file=None
    ):
        """ Do inference

        The inference can do two things:
        1. (Optional) Evaluate the model performance on a test set if given
        2. Generate node embeddings

        Parameters
        ----------
        data: GSgnnData
            The GraphStorm dataset
        loader : GSgnnLinkPredictionTestDataLoader
            The mini-batch sampler for link prediction task.
        edge_mask_for_gnn_embeddings : str
            The mask that indicates the edges used for computing GNN embeddings. By default,
            the dataloader uses the edges in the training graphs to compute GNN embeddings to
            avoid information leak for link prediction.
        node_id_mapping_file: str
            Path to the file storing node id mapping generated by the
            graph partition algorithm.
        """
        sys_tracker.check('start inferencing')
        self._model.eval()
        embs = do_full_graph_inference(self._model, data, fanout=loader.fanout,
                                       edge_mask=edge_mask_for_gnn_embeddings,
                                       task_tracker=self.task_tracker)
        sys_tracker.check('remap embeddings')
        device = self.device

        # remap gnn embeddings without writing to disk
        embs = remap_embeddings(embs, self.rank, 
            th.distributed.get_world_size(),
            device=device,
            node_id_mapping_file=node_id_mapping_file)

        return embs

    def get_textual_file_names_per_worker(
        self, 
        total_file_list, 
        local_rank, 
        world_size,
    ):
        """ Distribute textual files for embedding mapping.
        Each worker gets a list of file names, so that embedding mapping
        can be conducted in distributed way.

        Parameters
        ----------
        total_file_list: list of str
            List of all file names.
        local_rank : int
            rank for the local worker.
        world_size : int
            Number of workers.

        Returns
        -------
        list : list for the file names for the local worker.
        """
        num_files = len(total_file_list)
        part_len = num_files // world_size
        remainder = num_files % world_size
        part_start = part_len * local_rank + min(local_rank, remainder)
        part_end = part_start + part_len + (local_rank < remainder)
        part_len = part_end - part_start

        if part_len == 0:
            return []

        local_file_list = []
        for i in range(part_len):
            local_file_list.append(total_file_list[part_start+i])
        return local_file_list

    def map_text_embeds(
        self,
        queue,
        node_type,
        file_name,
        id_field,
        textual_field_list,
        disk_embeds,
        id_map,
        index,
        total_num,
        concat_field_name=False,
        split="train",
        chunk_size=10000,
        part=0,
        rank=0,
    ): 
        """ Map textual features with embeddings for each file.
        Embeddings are read from HDF5Array on disk,
        so no embeddings copy for each worker. A queue is maintained
        for each worker to store the samples. Joined files will be written
        to disk when queue size hits the chunk size. This is to make sure
        the same batch numbers during training of distillation.

        Parameters
        ----------
        queue: dict
            Store joined samples to be written to disk.
        node_type : str
            node type.
        file_name : str
            Name of the textual file.
        id_field : str
            Name of string ID field in textual data.
        textual_field_list : list of str
            List of textual field name to be considered.
        disk_embeds : HDF5Array
            Offset to embeddings map.
        id_map : dict
            String ID to offset map.
        index : int
            Index of current textual file
        total_num : int
            Number of local textual files
        concat_field_name : bool
            Whether to concat field name with textual features.
        split : str
            Split of current textual file.
        chunk_size : int
            Chunk size for the output joined file when writing to disk.
        part : int
            Partition index of the output joined file.
        rank : int
            Local rank.


        Returns
        -------
        queue: dict
            Store joined samples to be written to disk.
        part : int
            Partition index of the output joined file.
        """
        print ("Worker {}: Map {} textual features: {} of {}".format(self.rank, node_type, index, total_num))
        assert len(queue["node_ids"]) < chunk_size, "Queue already greater than chunk size"

        embed_offset = []

        with open(file_name, "r") as reader:
            for line in reader:
                # TODO (HZ): add the option for reading from parquet data
                json_data = json.loads(line)
                text = ""
                for i, f in enumerate(textual_field_list):
                    if f in json_data and json_data[f] is not None and str(json_data[f]):
                        if concat_field_name:
                            text += f + ": "
                        if not isinstance(json_data[f], str):
                            print (f"Warning: {f} is not a string feature")
                        field_value = str(json_data[f])
                        if i != len(textual_field_list) - 1:
                            text += field_value + " "
                        else:
                            text += field_value
                if text == "":
                    print (f"Warning: empty textual features for node {json_data[id_field]}, fill with node id")
                    text = str(json_data[id_field])
                queue["node_ids"].append(json_data[id_field])
                queue["textual_feats"].append(text)
                embed_offset.append(id_map[json_data[id_field]])

                # writing the joined data to disk when hits the chunk size
                if len(queue["node_ids"]) == chunk_size:
                    queue["embeddings"] = queue["embeddings"] + embed_offset
                    textual_embed_pddf = pd.DataFrame({
                        "ids": queue["node_ids"],
                        "textual_feats": queue["textual_feats"], 
                        "embeddings": disk_embeds[queue["embeddings"]].tolist()
                    }).set_index("ids")
                    outdir = os.path.join(os.path.dirname(os.path.dirname(file_name)), \
                        node_type + "_distill_data", split)
                    if not os.path.exists(outdir):
                        os.makedirs(outdir, exist_ok=True)
                    textual_embed_pddf.to_parquet(os.path.join(outdir, f"rank-{rank}-part-{part}.parquet"))
                    for k in queue.keys():
                        queue[k] = []
                    embed_offset = []
                    part += 1

        queue["embeddings"] = queue["embeddings"] + embed_offset
        return queue, part

    def distill(
        self, 
        node_type, 
        tsf_name, 
        gnn_embed_dim, 
        data_dir,
        batch_size,
        distill_lr,
        saved_path, 
        on_cpu=False,
    ):
        """ Distill function.
        Each worker initializes student model, data provider,
        and start training.

        Parameters
        ----------
        node_type: str
            node type.
        tsf_name : str
            Model name for Transformer-based student model.
        gnn_embed_dim : int
            Size of GNN embeddings.
        data_dir : str
            Directory for distillation data.
        batch_size : int
            Batch size for distillation.
        distill_lr : float
            Learning rate for distillation.
        saved_path : str
            Path to save the model.
        on_cpu : bool
            Whether the distillation will be conducted on cpu.
        """
        self.student = GSDistilledModel(tsf_name, node_type, gnn_embed_dim, pre_trained_name=self.config.pre_trained_name)
        dataloader_generator = DataloaderGenerator(tokenizer=self.student.tokenizer, 
            device=self.device, 
            batch_size=batch_size,
        )
        train_data_provider = DataProvider(
            dataloader_generator,
            dataset_path=os.path.join(data_dir, 'train'),
            local_rank=self.rank,
            world_size=th.distributed.get_world_size(),
            is_train=True,
        )
        eval_data_provider = DataProvider(
            dataloader_generator,
            dataset_path=os.path.join(data_dir, 'val'),
            local_rank=self.rank,
            world_size=th.distributed.get_world_size(),
            is_train=False,
        )

        # TODO (HZ): add flexibility to specify different optimizers
        optimizer = th.optim.Adam(self.student.parameters(), lr=distill_lr)
        self.student.to(self.device)
        student = DistributedDataParallel(self.student, device_ids=None if on_cpu else [self.device],
                                        output_device=None if on_cpu else self.device)

        index = 0
        global_step = 0
        while True:
            th.distributed.barrier()
            print (f"Train {index + 1}-th shard by trainer {self.rank}")
            complete, global_step = self.train_shard(
                global_step=global_step,
                model=student, 
                optimizer=optimizer, 
                train_dataset_provider=train_data_provider, 
                eval_dataset_provider=eval_data_provider,
                saved_path=saved_path,
            )
            is_first_iteration = False

            index += 1
            if complete:
                break

    def save_student_model(self, model, saved_path, global_step):
        """ Save student model

        Parameters
        ----------
        model : GSDistilledModel
            Distilled student model.
        saved_path : str
            Path to save the model.
        global_step : int
            Global step of the model checkpoint.
        """
        th.distributed.barrier()
        if self.rank == 0:
            os.makedirs(os.path.join(saved_path, model.module.node_type, f"checkpoint-{global_step}"), exist_ok=True)
            config_file_loc = os.path.join(saved_path, model.module.node_type, f"checkpoint-{global_step}", "config.yaml")
            model_file_loc = os.path.join(saved_path, model.module.node_type, f"checkpoint-{global_step}", "pytorch_model.bin")

            def to_dict(config):
                """
                Serializes this instance to a Python dictionary.
                """
                json_string = json.dumps(
                    config.__dict__,
                    default=lambda o: getattr(o, "__dict__", str(o)),
                    indent=2,
                    sort_keys=True,
                )
                output = json.loads(json_string)
                return output

            config_dict = to_dict(self.config)
            with open(config_file_loc, "w", encoding="utf-8") as of:
                json.dump(config_dict, of, indent=2, sort_keys=True)
            # TODO (HZ): need to test if the saved model can be successfully loaded
            th.save(model.state_dict(), model_file_loc)

        return True

    def to_device(self, inputs, device='cuda'):
        """ Move the mini batch to corresponding device.

        Parameters
        ----------
        inputs: dict of tensor
            A batch from dataloader.
        device : str
            Name for the local device.

        Returns
        -------
        dict of tensor : A batch on the specified device.
        """
        if inputs is None:
            return None
        elif isinstance(inputs, th.Tensor):
            return inputs.to(device)
        elif isinstance(inputs, dict):
            outputs = {}
            for k, v in inputs.items():
                outputs[k] = self.to_device(v, device=device)
        elif isinstance(inputs, (list, tuple)):
            outputs = []
            for v in inputs:
                outputs.append(self.to_device(v, device=device))
        else:
            raise NotImplementedError
        return outputs

    def eval(self, model, eval_dataset_provider, global_step):
        """ Evaluate student model on validation set.
        The metric are mean square error (MSE).

        Parameters
        ----------
        model : GSDistilledModel
            Distilled student model.
        eval_dataset_provider : DataProvider
            Data provider for validation data.
        global_step : int
            Global step of the model checkpoint.
        """
        model.eval()
        index = 0
        batch_index = 0
        total_loss = 0
        while True:
            dataset_iterator, _ = eval_dataset_provider.get_shard()
            if dataset_iterator is None:
                break
            print (f"Eval {index + 1}-th shard by trainer {self.rank}")
            with th.no_grad():
                for batch_num, batch in enumerate(dataset_iterator):
                    batch = self.to_device(batch, self.device)  # Move to device
                    loss = model.module(batch["input_ids"],
                        batch["attention_mask"],
                        batch["labels"])
                    total_loss += loss.item()
                    batch_index += 1
            index += 1

        mean_total_loss = total_loss / batch_index
        print (f"Eval MSE at step {global_step}: {mean_total_loss}")
        model.train()
        eval_dataset_provider.release_shard()

    def train_shard(
        self, 
        global_step, 
        model, 
        optimizer, 
        train_dataset_provider, 
        eval_dataset_provider, 
        saved_path,
    ):
        """
        Train using one shard from train_dataset_provider.
        Parameters
        ----------
        global_step : int
            Global step of the model checkpoint.
        model : GSDistilledModel
            Distilled student model.
        optimizer : torch optimizer
            optimizer for distillation.
        train_dataset_provider : DataProvider
            Data provider for training data.
        eval_dataset_provider : DataProvider
            Data provider for validation data.
        saved_path : str
            Path to save the model.
    
        Returns
        -------
        bool : whether to stop distillation.
        int : Global step of the model checkpoint.
        """
        dataset_iterator, _ = train_dataset_provider.get_shard()
        # TODO (HZ): support prefetch to speed up the training
        model.train()
        shard_loss = 0
        complete = False

        for batch_num, batch in enumerate(dataset_iterator):
            try:
                batch = self.to_device(batch, self.device)  # Move to device

                loss = model(batch["input_ids"],
                    batch["attention_mask"],
                    batch["labels"])

                shard_loss += loss.item()
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                mean_shard_loss = shard_loss / (batch_num + 1)
                if global_step % 20 == 0:
                    print (
                        f"Loss for shard {train_dataset_provider.get_shard_name()}"
                        f" at step {global_step} = {mean_shard_loss}"
                        f" from trainer {self.rank}"
                    )

                if global_step % self.config.save_student_interval == 0 and global_step != 0:
                    self.save_student_model(model, saved_path, global_step)

                if global_step % self.config.eval_student_interval == 0 and global_step != 0:
                    th.distributed.barrier()
                    # TODO (HZ): implement distributed evaluation by communicating with all trainers
                    if self.rank == 0:
                        self.eval(model, eval_dataset_provider, global_step)
                    th.distributed.barrier()

                if global_step == self.config.max_global_step[model.module.node_type]:
                    complete = True
                    break
                global_step += 1
            except StopIteration:
                continue

        # release the memory
        train_dataset_provider.release_shard()

        return complete, global_step

    def setup_device(self, device):
        """ Set up the device for the distillation.

        The CUDA device is set up based on the local rank.

        Parameters
        ----------
        device :
            The device for distillation.
        """
        self._device = th.device(device)
        self._model = self._model.to(self.device)

    def setup_task_tracker(self, task_tracker):
        """ Set the task tracker.

        Parameters
        ----------
        task_tracker : GSTaskTracker
            The task tracker
        """
        if self.evaluator is not None:
            self.evaluator.setup_task_tracker(task_tracker)
        self._task_tracker = task_tracker

    @property
    def task_tracker(self):
        """ Get the task tracker associated with the inference.
        """
        return self._task_tracker

    @property
    def rank(self):
        """ Get the rank for the distillation.
        """
        return self._rank

    @property
    def config(self):
        """ Get the config for the distillation.
        """
        return self._config

    @property
    def device(self):
        """ The device associated with the inference.
        """
        return self._device