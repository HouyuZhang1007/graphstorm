"""
    Copyright 2023 Contributors

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

    Distill wrapper.
"""

# from .graphstorm_infer import GSInfer
import torch as th
from torch.nn.parallel import DistributedDataParallel
import os
import json

from ..model.gnn import do_full_graph_inference
from ..utils import sys_tracker
from .utils import remap_embeddings

from ..model import GSDistilledModel
from ..dataloading import DataloaderGenerator, DataProvider

class GSdistiller():
    """
    """
    def __init__(self, model, data_provider, rank, config):
        self._rank = rank
        self._model = model
        self._device = -1
        self._evaluator = None
        self._task_tracker = None
        self._config = config

    def infer(self, data, loader,
            edge_mask_for_gnn_embeddings='train_mask',
            node_id_mapping_file=None):
        """ Do inference

        The inference can do two things:
        1. (Optional) Evaluate the model performance on a test set if given
        2. Generate node embeddings

        Parameters
        ----------
        data: GSgnnData
            The GraphStorm dataset
        loader : GSgnnLinkPredictionTestDataLoader
            The mini-batch sampler for link prediction task.
        edge_mask_for_gnn_embeddings : str
            The mask that indicates the edges used for computing GNN embeddings. By default,
            the dataloader uses the edges in the training graphs to compute GNN embeddings to
            avoid information leak for link prediction.
        node_id_mapping_file: str
            Path to the file storing node id mapping generated by the
            graph partition algorithm.
        """
        sys_tracker.check('start inferencing')
        self._model.eval()
        embs = do_full_graph_inference(self._model, data, fanout=loader.fanout,
                                       edge_mask=edge_mask_for_gnn_embeddings,
                                       task_tracker=self.task_tracker)
        sys_tracker.check('remap embeddings')
        device = self.device

        # remap gnn embeddings without writing to disk
        embs = remap_embeddings(embs, self.rank, 
            th.distributed.get_world_size(),
            device=device,
            node_id_mapping_file=node_id_mapping_file)

        return embs

    def distill(
        self, 
        node_type, 
        tsf_name, 
        gnn_embed_dim, 
        data_dir,
        batch_size,
        distill_lr,
        saved_path, 
        on_cpu=False,
    ):
        self.student = GSDistilledModel(tsf_name, node_type, gnn_embed_dim, pre_trained_name=self.config.pre_trained_name)
        dataloader_generator = DataloaderGenerator(tokenizer=self.student.tokenizer, 
            device=self.device, 
            batch_size=batch_size,
        )
        train_data_provider = DataProvider(
            dataloader_generator,
            dataset_path=os.path.join(data_dir, 'train'),
            local_rank=self.rank,
            world_size=th.distributed.get_world_size(),
            is_train=True,
        )
        eval_data_provider = DataProvider(
            dataloader_generator,
            dataset_path=os.path.join(data_dir, 'val'),
            local_rank=self.rank,
            world_size=th.distributed.get_world_size(),
            is_train=False,
        )
        # TODO: add eval dataprovider
        optimizer = th.optim.Adam(self.student.parameters(), lr=distill_lr)
        self.student.to(self.device)
        student = DistributedDataParallel(self.student, device_ids=None if on_cpu else [self.device],
                                        output_device=None if on_cpu else self.device)

        index = 0
        global_step = 0
        while True:
            th.distributed.barrier()
            print (f"Train {index + 1}-th shard by trainer {self.rank}")
            complete, global_step = self.train_shard(
                global_step=global_step,
                model=student, 
                optimizer=optimizer, 
                train_dataset_provider=train_data_provider, 
                eval_dataset_provider=eval_data_provider,
                saved_path=saved_path,
            )
            is_first_iteration = False

            # logger.info(f"Time for {index + 1}-th shard: {post - pre} seconds")

            index += 1
            if complete:
                break


            # complete, global_step, current_data_sample_count, overall_time = self.train_shard(
            #     args=args, 
            #     model=model, 
            #     optimizer=optimizer, 
            #     train_dataset_provider=dataset_provider, 
            #     early_stopping=early_stopping, 
            #     is_first_iteration=is_first_iteration,
            #     global_step=global_step, 
            #     current_data_sample_count=current_data_sample_count, 
            #     last_global_step_from_restore=last_global_step_from_restore,
            #     overall_time=overall_time,
            #     sample_scheduler=sample_scheduler,
            #     )
            # is_first_iteration = False
            # post = time.perf_counter()

            # logger.info(f"Time for {index + 1}-th shard: {post - pre} seconds")

            # index += 1
            # if complete:
            #     break

    def save_student_model(self, model, saved_path, global_step):
        th.distributed.barrier()
        if self.rank == 0:
            os.makedirs(os.path.join(saved_path, model.module.node_type, f"checkpoint-{global_step}"), exist_ok=True)
            config_file_loc = os.path.join(saved_path, model.module.node_type, f"checkpoint-{global_step}", "config.yaml")
            model_file_loc = os.path.join(saved_path, model.module.node_type, f"checkpoint-{global_step}", "pytorch_model.bin")

            def to_dict(config):
                """
                Serializes this instance to a Python dictionary.
                """

                json_string = json.dumps(
                    config.__dict__,
                    default=lambda o: getattr(o, "__dict__", str(o)),
                    indent=2,
                    sort_keys=True,
                )
                output = json.loads(json_string)
                return output

            config_dict = to_dict(self.config)
            with open(config_file_loc, "w", encoding="utf-8") as of:
                json.dump(config_dict, of, indent=2, sort_keys=True)
            th.save(model.state_dict(), model_file_loc)

        return True

    def to_device(self, inputs, device='cuda'):
        if inputs is None:
            return None
        elif isinstance(inputs, th.Tensor):
            return inputs.to(device)
        elif isinstance(inputs, dict):
            outputs = {}
            for k, v in inputs.items():
                outputs[k] = self.to_device(v, device=device)
        elif isinstance(inputs, (list, tuple)):
            outputs = []
            for v in inputs:
                outputs.append(self.to_device(v, device=device))
        else:
            raise NotImplementedError
        return outputs

    def eval(self, model, eval_dataset_provider, global_step):
        model.eval()
        index = 0
        batch_index = 0
        total_loss = 0
        while True:
            dataset_iterator, _ = eval_dataset_provider.get_shard()
            if dataset_iterator is None:
                break
            print (f"Eval {index + 1}-th shard by trainer {self.rank}")
            with th.no_grad():
                for batch_num, batch in enumerate(dataset_iterator):
                    batch = self.to_device(batch, self.device)  # Move to device
                    loss = model.module(batch["input_ids"],
                        batch["attention_mask"],
                        batch["labels"])
                    total_loss += loss.item()
                    batch_index += 1
            index += 1

        mean_total_loss = total_loss / batch_index
        print (f"Eval MSE at step {global_step}: {mean_total_loss}")
        model.train()
        eval_dataset_provider.release_shard()
        return True

    def train_shard(
        self, 
        global_step, 
        model, 
        optimizer, 
        train_dataset_provider, 
        eval_dataset_provider, 
        saved_path,
    ):
        """
        Train using one shard from train_dataset_provider.
        Input:
            args: User args object obtained via parse_args.
            model: Deepspeed model to be trained.
            optimizer: Optimizer to train the deepspeed model.
            train_dataset_provider: Dataset provider to train the model.
            early_stopping: EarlyStopping object to be used.
            is_first_iteration: True/False. Will load offset value and rewind when set to True
                which is used when loading a checkpoint.
        Output:
            Returns True if stopping criteria was met else False.
        """
        dataset_iterator, _ = train_dataset_provider.get_shard()
        # TODO (HZ): to support prefetch

        model.train()

        shard_loss = 0
        complete = False

        for batch_num, batch in enumerate(dataset_iterator):
            try:
                batch = self.to_device(batch, self.device)  # Move to device

                loss = model(batch["input_ids"],
                    batch["attention_mask"],
                    batch["labels"])

                shard_loss += loss.item()
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                mean_shard_loss = shard_loss / (batch_num + 1)
                if global_step % 20 == 0:
                    print (
                        f"Loss for shard {train_dataset_provider.get_shard_name()}"
                        f" at step {global_step} = {mean_shard_loss}"
                        f" from trainer {self.rank}"
                    )

                if global_step % self.config.save_student_interval == 0 and global_step != 0:
                    self.save_student_model(model, saved_path, global_step)

                if global_step % self.config.eval_student_interval == 0 and global_step != 0:
                    th.distributed.barrier()
                    # TODO (HZ): implement distributed evaluation by communicating with all trainers
                    if self.rank == 0:
                        self.eval(model, eval_dataset_provider, global_step)
                    th.distributed.barrier()

                if global_step == self.config.max_global_step[model.module.node_type]:
                    complete = True
                    break

                global_step += 1
            except StopIteration:
                continue

        train_dataset_provider.release_shard()

        return complete, global_step

    def setup_device(self, device):
        """ Set up the device for the distillation.

        The CUDA device is set up based on the local rank.

        Parameters
        ----------
        device :
            The device for distillation.
        """
        self._device = th.device(device)
        self._model = self._model.to(self.device)

    def setup_task_tracker(self, task_tracker):
        """ Set the task tracker.

        Parameters
        ----------
        task_tracker : GSTaskTracker
            The task tracker
        """
        if self.evaluator is not None:
            self.evaluator.setup_task_tracker(task_tracker)
        self._task_tracker = task_tracker

    def setup_evaluator(self, evaluator):
        """ Set the evaluator
        """
        if self.task_tracker is not None:
            evaluator.setup_task_tracker(self.task_tracker)
        self._evaluator = evaluator

    def log_print_metrics(self, val_score, test_score, dur_eval, total_steps, train_score=None):
        """
        This function prints and logs all the metrics for evaluation

        Parameters
        ----------
        train_score: dict
            Training score
        val_score: dict
            Validation score
        test_score: dict
            Test score
        dur_eval:
            Total evaluation time
        total_steps: int
            The corresponding step/iteration
        """
        if self.task_tracker is None:
            return

        best_val_score = self.evaluator.best_val_score
        best_test_score = self.evaluator.best_test_score
        best_iter_num = self.evaluator.best_iter_num
        self.task_tracker.log_iter_metrics(self.evaluator.metric,
                train_score=train_score, val_score=val_score,
                test_score=test_score, best_val_score=best_val_score,
                best_test_score=best_test_score, best_iter_num=best_iter_num,
                eval_time=dur_eval, total_steps=total_steps)

    @property
    def rank(self):
        """ Get the rank for the distillation.
        """
        return self._rank

    @property
    def config(self):
        """ Get the config for the distillation.
        """
        return self._config

    @property
    def evaluator(self):
        """ Get the evaluator associated with the inference.
        """
        return self._evaluator

    @property
    def task_tracker(self):
        """ Get the task tracker associated with the inference.
        """
        return self._task_tracker

    @property
    def device(self):
        """ The device associated with the inference.
        """
        return self._device