"""
        Copyright 2023 Contributors

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

    Utils for distillation.
"""
import torch as th
import pandas as pd
import dgl
import os

from ..model.utils import _exchange_node_id_mapping, _get_data_range, LazyDistTensor

def read_nid(node_type, dist_dgl_graph_dir):
    """ Read node ID map.

        Parameters
        ----------
        node_type: str
            Node tpye.
        dist_dgl_graph_dir : str
            Path for partitioned graph.

        Returns
        -------
        list of str : Map of node IDs.
        """
    nid_file = os.path.join(dist_dgl_graph_dir, f"{node_type}_id_remap.parquet")
    print (f"reading nid-{node_type}: {nid_file}")
    node_ids = pd.read_parquet(nid_file)["orig"].tolist()
    print (f"read nid of size {len(node_ids)}")
    return node_ids

def remap_embeddings(embeddings, local_rank, world_size,
    device=th.device('cpu'), node_id_mapping_file=None):
    """ Save embeddings in a distributed way

        Parameters
        ----------
        embeddings : DistTensor
            Embeddings to save
        local_rank : int
            Local rank
        world_size : int
            World size in a distributed env.
        device: torch device
            Device used for all_to_allv data exchange. For gloo backend
            we store data in CPU, For nccl backend, we need to store
            data in GPU.
        node_id_mapping_file: str
            Path to the file storing node id mapping generated by the
            graph partition algorithm.

        Returns
        -------
        dict or dgl.distributed.DistTensor : GNN embeddings
    """
    # TODO (HZ): modify and use the existing function from model.utils
    th.distributed.barrier()

    assert local_rank < world_size
    # Node ID mapping won't be very large if number of nodes is
    # less than 10 billion. An ID mapping of 10 billion nodes
    # will take around 80 GByte.
    if node_id_mapping_file is not None:
        if isinstance(embeddings, (dgl.distributed.DistTensor, LazyDistTensor)):
            # only host 0 will load node id mapping from disk
            node_id_mapping = th.load(node_id_mapping_file) \
                if local_rank == 0 else None

            nid_mapping = _exchange_node_id_mapping(
                local_rank, world_size, device, node_id_mapping, len(embeddings))
        elif isinstance(embeddings, dict):
            nid_mapping = {}
            # only host 0 will load node id mapping from disk
            node_id_mappings = th.load(node_id_mapping_file) \
                if local_rank == 0 else None

            for name, emb in embeddings.items():
                if local_rank == 0:
                    assert name in node_id_mappings, \
                        f"node id mapping for ntype {name} should exists"
                    node_id_mapping = node_id_mappings[name]
                else:
                    node_id_mapping = None

                nid_mapping[name] = _exchange_node_id_mapping(
                    local_rank, world_size, device, node_id_mapping, len(emb))
        else:
            nid_mapping = None
    else:
        nid_mapping = None

    if isinstance(embeddings, (dgl.distributed.DistTensor, LazyDistTensor)):
        if nid_mapping is not None:
            embeddings[list(range(start, end))] = embeddings[nid_mapping]
    elif isinstance(embeddings, dict):
        # We need to duplicate the dict so that the input argument is not changed.
        embeddings = dict(embeddings.items())
        for name, emb in embeddings.items():
            if isinstance(emb, (dgl.distributed.DistTensor, LazyDistTensor)):
                start, end = _get_data_range(local_rank, world_size, len(emb))
                if nid_mapping is not None:
                    emb[list(range(start, end))] = emb[nid_mapping[name]]
                embeddings[name] = emb
            th.distributed.barrier()

    return embeddings